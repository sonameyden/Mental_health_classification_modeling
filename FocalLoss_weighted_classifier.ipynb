{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MjM7rFdGJuiR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bDnBtPBKom1",
        "outputId": "537685ba-60b9-4ee2-b4b1-5533978d71c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JJZ0TtzzN0La"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/combined_data.csv\")\n",
        "ndata = data.dropna().reset_index(drop=True)\n",
        "\n",
        "text = ndata['text']\n",
        "labels = ndata['mental_state'].values\n",
        "label_map = {emotion: idx for idx, emotion in enumerate(np.unique(labels))}\n",
        "num_labels = len(label_map)\n",
        "y_encoded = np.array([label_map[l] for l in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cOCl-hS7FbdT"
      },
      "outputs": [],
      "source": [
        "embeddings = np.load('/content/drive/MyDrive/roberta_embeddings_yess_2025.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bQpCachI22k",
        "outputId": "7f5926a6-aa76-4fbe-c2c4-908a8507a026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please ensure you have loaded your data:\n",
            "- ndata: your cleaned dataframe\n",
            "- text: text column from ndata\n",
            "- embeddings: your RoBERTa embeddings\n",
            "- labels: your mental_state labels\n",
            "Original class distribution:\n",
            "anxiety             : 17054 (13.8%)\n",
            "normal              : 16343 (13.3%)\n",
            "depression          : 48958 (39.7%)\n",
            "suicidal            : 25243 (20.5%)\n",
            "stress              :  2587 (2.1%)\n",
            "bipolar             :  7765 (6.3%)\n",
            "personality disorder:  1077 (0.9%)\n",
            "lonely              :  2820 (2.3%)\n",
            "ptsd                :  1397 (1.1%)\n",
            "\n",
            "Training set shape: (98595, 1836)\n",
            "Training set class distribution:\n",
            "suicidal            : 20194 (20.5%)\n",
            "depression          : 39166 (39.7%)\n",
            "bipolar             :  6212 (6.3%)\n",
            "normal              : 13074 (13.3%)\n",
            "anxiety             : 13643 (13.8%)\n",
            "stress              :  2070 (2.1%)\n",
            "ptsd                :  1118 (1.1%)\n",
            "lonely              :  2256 (2.3%)\n",
            "personality disorder:   862 (0.9%)\n",
            "\n",
            "Class weights:\n",
            "anxiety             : 0.8030\n",
            "bipolar             : 1.7635\n",
            "depression          : 0.2797\n",
            "lonely              : 4.8559\n",
            "normal              : 0.8379\n",
            "personality disorder: 12.7088\n",
            "ptsd                : 9.7987\n",
            "stress              : 5.2923\n",
            "suicidal            : 0.5425\n",
            "\n",
            "============================================================\n",
            "Starting Cross-Validation with LightGBM + Focal Loss...\n",
            "============================================================\n",
            "\n",
            "==================== Fold 1 / 5 ====================\n",
            "\n",
            "Focal Loss Iteration 1/3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttrain's multi_logloss: 0.0984494\tval's multi_logloss: 0.337687\n",
            "[200]\ttrain's multi_logloss: 0.0695638\tval's multi_logloss: 0.328806\n",
            "[300]\ttrain's multi_logloss: 0.0542813\tval's multi_logloss: 0.328374\n",
            "Early stopping, best iteration is:\n",
            "[259]\ttrain's multi_logloss: 0.0597327\tval's multi_logloss: 0.327618\n",
            "Iteration 1 - Train Focal Loss: 0.0134, Val Focal Loss: 0.0500\n",
            "\n",
            "Focal Loss Iteration 2/3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's multi_logloss: 0.857023\tval's multi_logloss: 2.26127\n",
            "Iteration 2 - Train Focal Loss: 0.4702, Val Focal Loss: 0.4793\n",
            "\n",
            "Focal Loss Iteration 3/3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttrain's multi_logloss: 0.151331\tval's multi_logloss: 0.637969\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttrain's multi_logloss: 0.0370015\tval's multi_logloss: 0.477413\n",
            "Iteration 3 - Train Focal Loss: 0.0645, Val Focal Loss: 0.0890\n",
            "Fold 1 - Accuracy: 0.8356, F1: 0.8338, Loss: 0.4776, Focal Loss: 0.0890\n",
            "\n",
            "==================== Fold 2 / 5 ====================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Install required packages (run this first if not installed)\n",
        "# !pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "class FocalLossLGBM:\n",
        "    \"\"\"\n",
        "    Implementation using iterative focal loss reweighting for LightGBM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def calculate_focal_loss(self, y_true, y_pred_proba):\n",
        "        \"\"\"\n",
        "        Calculate focal loss for evaluation\n",
        "        \"\"\"\n",
        "        # Get the probability of the true class for each sample\n",
        "        true_class_probs = y_pred_proba[np.arange(len(y_true)), y_true]\n",
        "\n",
        "        # Clip probabilities to avoid log(0)\n",
        "        true_class_probs = np.clip(true_class_probs, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        # Calculate focal loss\n",
        "        focal_loss = -self.alpha * (1 - true_class_probs) ** self.gamma * np.log(true_class_probs)\n",
        "\n",
        "        return np.mean(focal_loss)\n",
        "\n",
        "    def apply_focal_weighting(self, y_true, y_pred_proba):\n",
        "        \"\"\"\n",
        "        Apply focal loss weighting to sample weights\n",
        "        \"\"\"\n",
        "        # Get the probability of the true class for each sample\n",
        "        true_class_probs = y_pred_proba[np.arange(len(y_true)), y_true]\n",
        "\n",
        "        # Clip probabilities to avoid numerical issues\n",
        "        true_class_probs = np.clip(true_class_probs, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        # Calculate focal weights\n",
        "        focal_weights = self.alpha * (1 - true_class_probs) ** self.gamma\n",
        "\n",
        "        return focal_weights\n",
        "\n",
        "    def train_with_focal_reweighting(self, X_train, y_train, X_val, y_val,\n",
        "                                   initial_weights, lgb_params, n_iterations=3):\n",
        "        \"\"\"\n",
        "        Train model with iterative focal loss reweighting\n",
        "        \"\"\"\n",
        "        current_weights = initial_weights.copy()\n",
        "\n",
        "        for iteration in range(n_iterations):\n",
        "            print(f\"\\nFocal Loss Iteration {iteration + 1}/{n_iterations}\")\n",
        "\n",
        "            # Create datasets with current weights\n",
        "            train_data = lgb.Dataset(X_train, label=y_train, weight=current_weights)\n",
        "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "            # Train model\n",
        "            model = lgb.train(\n",
        "                lgb_params,\n",
        "                train_data,\n",
        "                valid_sets=[train_data, val_data],\n",
        "                valid_names=['train', 'val'],\n",
        "                num_boost_round=1000,\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=50),\n",
        "                    lgb.log_evaluation(period=100)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Get predictions on training set\n",
        "            y_train_pred_proba = model.predict(X_train, num_iteration=model.best_iteration)\n",
        "\n",
        "            # Calculate focal weights\n",
        "            focal_weights = self.apply_focal_weighting(y_train, y_train_pred_proba)\n",
        "\n",
        "            # Combine with initial weights\n",
        "            current_weights = initial_weights * focal_weights\n",
        "\n",
        "            # Normalize weights\n",
        "            current_weights = current_weights / np.mean(current_weights) * len(current_weights)\n",
        "\n",
        "            # Print focal loss for this iteration\n",
        "            train_focal_loss = self.calculate_focal_loss(y_train, y_train_pred_proba)\n",
        "            val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "            val_focal_loss = self.calculate_focal_loss(y_val, val_pred_proba)\n",
        "            print(f\"Iteration {iteration + 1} - Train Focal Loss: {train_focal_loss:.4f}, Val Focal Loss: {val_focal_loss:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "def create_class_weights(y_train, method='balanced'):\n",
        "    \"\"\"\n",
        "    Create class weights for imbalanced dataset\n",
        "    \"\"\"\n",
        "    class_counts = np.bincount(y_train)\n",
        "    total_samples = len(y_train)\n",
        "    n_classes = len(class_counts)\n",
        "\n",
        "    if method == 'balanced':\n",
        "        # Standard balanced weights\n",
        "        class_weights = total_samples / (n_classes * class_counts)\n",
        "    elif method == 'sqrt':\n",
        "        # Square root balanced weights (less aggressive)\n",
        "        class_weights = np.sqrt(total_samples / (n_classes * class_counts))\n",
        "    else:\n",
        "        # No weighting\n",
        "        class_weights = np.ones(n_classes)\n",
        "\n",
        "    return class_weights\n",
        "\n",
        "\n",
        "def focal_loss_lgb(y_pred, y_true, alpha=0.25, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Calculate focal loss for multiclass classification\n",
        "    This is used as a custom evaluation metric\n",
        "    \"\"\"\n",
        "    y_true = y_true.get_label().astype(int)\n",
        "    num_classes = len(np.unique(y_true))\n",
        "\n",
        "    # Reshape predictions to (n_samples, n_classes)\n",
        "    y_pred = y_pred.reshape(num_classes, -1).T\n",
        "\n",
        "    # Convert to probabilities using softmax\n",
        "    exp_pred = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
        "    prob = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n",
        "\n",
        "    # Clip probabilities to avoid log(0)\n",
        "    prob = np.clip(prob, 1e-15, 1 - 1e-15)\n",
        "\n",
        "    # Get probabilities for true classes\n",
        "    true_prob = prob[np.arange(len(y_true)), y_true]\n",
        "\n",
        "    # Calculate focal loss\n",
        "    focal_loss = -alpha * (1 - true_prob) ** gamma * np.log(true_prob)\n",
        "\n",
        "    return 'focal_loss', np.mean(focal_loss), False\n",
        "\n",
        "\n",
        "# Load your data (assuming you have this part ready)\n",
        "# For demonstration, I'll create placeholder variables\n",
        "# Replace these with your actual data loading\n",
        "# data = pd.read_csv(\"/content/drive/MyDrive/combined_data.csv\")\n",
        "# ndata = data.dropna().reset_index(drop=True)\n",
        "# text = ndata['text']\n",
        "# labels = ndata['mental_state'].values\n",
        "# embeddings = np.load('/content/drive/MyDrive/roberta_embeddings_yess_2025.npy')\n",
        "\n",
        "# Uncomment the following lines and replace with your actual data\n",
        "# For now, I'll use placeholder comments\n",
        "print(\"Please ensure you have loaded your data:\")\n",
        "print(\"- ndata: your cleaned dataframe\")\n",
        "print(\"- text: text column from ndata\")\n",
        "print(\"- embeddings: your RoBERTa embeddings\")\n",
        "print(\"- labels: your mental_state labels\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(ndata['mental_state'])\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "original_dist = Counter(labels_encoded)\n",
        "for class_idx, count in original_dist.items():\n",
        "    print(f\"{le.classes_[class_idx]:<20}: {count:5d} ({count/len(labels_encoded)*100:.1f}%)\")\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train_indices, X_test_indices, y_train_final, y_test_final = train_test_split(\n",
        "    range(len(text)), labels_encoded, test_size=0.2, stratify=labels_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split text and embeddings\n",
        "text_train = [text.iloc[i] for i in X_train_indices]\n",
        "text_test = [text.iloc[i] for i in X_test_indices]\n",
        "embeddings_train = embeddings[X_train_indices]\n",
        "embeddings_test = embeddings[X_test_indices]\n",
        "\n",
        "# Fit TF-IDF ONLY on training data\n",
        "tfidf = TfidfVectorizer(max_features=300)\n",
        "tfidf_features_train = tfidf.fit_transform(text_train).toarray()\n",
        "tfidf_features_test = tfidf.transform(text_test).toarray()\n",
        "\n",
        "# Combine features for training and test sets\n",
        "X_train_full = np.hstack([embeddings_train, tfidf_features_train])\n",
        "X_test_full = np.hstack([embeddings_test, tfidf_features_test])\n",
        "\n",
        "# Fit scaler ONLY on training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train_scaled.shape}\")\n",
        "print(\"Training set class distribution:\")\n",
        "train_dist = Counter(y_train_final)\n",
        "for class_idx, count in train_dist.items():\n",
        "    print(f\"{le.classes_[class_idx]:<20}: {count:5d} ({count/len(y_train_final)*100:.1f}%)\")\n",
        "\n",
        "# Create class weights\n",
        "class_weights = create_class_weights(y_train_final, method='balanced')\n",
        "sample_weights = np.array([class_weights[y] for y in y_train_final])\n",
        "\n",
        "print(f\"\\nClass weights:\")\n",
        "for i, weight in enumerate(class_weights):\n",
        "    print(f\"{le.classes_[i]:<20}: {weight:.4f}\")\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(np.unique(y_train_final))\n",
        "\n",
        "# LightGBM parameters optimized for imbalanced classification\n",
        "lgb_params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': num_classes,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': ['multi_logloss'],\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_weight': 10,\n",
        "    'min_child_samples': 20,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 0.1,\n",
        "    'class_weight': 'balanced',  # Built-in class weighting\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': -1,\n",
        "    'force_col_wise': True,\n",
        "}\n",
        "\n",
        "# Initialize Focal Loss Trainer\n",
        "focal_loss_trainer = FocalLossLGBM(alpha=0.25, gamma=2.0)\n",
        "\n",
        "# Cross-validation setup\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "fold_metrics = []\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Starting Cross-Validation with LightGBM + Focal Loss...\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train_final)):\n",
        "    print(f\"\\n{'='*20} Fold {fold+1} / {n_splits} {'='*20}\")\n",
        "\n",
        "    X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "    y_fold_train, y_fold_val = y_train_final[train_idx], y_train_final[val_idx]\n",
        "\n",
        "    # Create initial sample weights for this fold\n",
        "    fold_weights = np.array([class_weights[y] for y in y_fold_train])\n",
        "\n",
        "    # Train model with focal loss reweighting\n",
        "    model = focal_loss_trainer.train_with_focal_reweighting(\n",
        "        X_fold_train, y_fold_train, X_fold_val, y_fold_val,\n",
        "        fold_weights, lgb_params, n_iterations=3\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = model.predict(X_fold_val, num_iteration=model.best_iteration)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    # Calculate focal loss for validation\n",
        "    focal_loss_val = focal_loss_trainer.calculate_focal_loss(y_fold_val, y_pred_proba)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_fold_val, y_pred)\n",
        "    precision = precision_score(y_fold_val, y_pred, average='weighted')\n",
        "    recall = recall_score(y_fold_val, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_fold_val, y_pred, average='weighted')\n",
        "    val_loss = log_loss(y_fold_val, y_pred_proba)\n",
        "\n",
        "    print(f\"Fold {fold+1} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Loss: {val_loss:.4f}, Focal Loss: {focal_loss_val:.4f}\")\n",
        "\n",
        "    fold_metrics.append({\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'loss': val_loss,\n",
        "        'focal_loss': focal_loss_val\n",
        "    })\n",
        "\n",
        "# Print average metrics across folds\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Cross-Validation Results (LightGBM + Focal Loss):\")\n",
        "print(\"=\"*60)\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1', 'loss', 'focal_loss']:\n",
        "    values = [fm[metric] for fm in fold_metrics]\n",
        "    print(f\"{metric.capitalize():<12}: {np.mean(values):.4f} \u00b1 {np.std(values):.4f}\")\n",
        "\n",
        "# Train final model on full training set\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Final Model with LightGBM + Focal Loss\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train final model with focal loss reweighting\n",
        "final_model = focal_loss_trainer.train_with_focal_reweighting(\n",
        "    X_train_scaled, y_train_final, X_test_scaled, y_test_final,\n",
        "    sample_weights, lgb_params, n_iterations=3\n",
        ")\n",
        "\n",
        "# Predictions on test set\n",
        "y_test_pred_proba = final_model.predict(X_test_scaled, num_iteration=final_model.best_iteration)\n",
        "y_test_pred = np.argmax(y_test_pred_proba, axis=1)\n",
        "\n",
        "# Calculate focal loss for test set\n",
        "test_focal_loss = focal_loss_trainer.calculate_focal_loss(y_test_final, y_test_pred_proba)\n",
        "\n",
        "# Test metrics\n",
        "test_accuracy = accuracy_score(y_test_final, y_test_pred)\n",
        "test_precision = precision_score(y_test_final, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test_final, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test_final, y_test_pred, average='weighted')\n",
        "test_loss = log_loss(y_test_final, y_test_pred_proba)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Test Set Performance (LightGBM + Focal Loss)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Log Loss: {test_loss:.4f}\")\n",
        "print(f\"Focal Loss: {test_focal_loss:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_final, y_test_pred, target_names=le.classes_, digits=4))\n",
        "\n",
        "# Per-class F1 scores\n",
        "print(\"\\nPer-class F1 Scores:\")\n",
        "f1_per_class = f1_score(y_test_final, y_test_pred, average=None)\n",
        "for i, f1_class in enumerate(f1_per_class):\n",
        "    print(f\"{le.classes_[i]:<20}: {f1_class:.4f}\")\n",
        "\n",
        "# Per-class precision and recall\n",
        "print(\"\\nPer-class Precision:\")\n",
        "precision_per_class = precision_score(y_test_final, y_test_pred, average=None)\n",
        "for i, prec_class in enumerate(precision_per_class):\n",
        "    print(f\"{le.classes_[i]:<20}: {prec_class:.4f}\")\n",
        "\n",
        "print(\"\\nPer-class Recall:\")\n",
        "recall_per_class = recall_score(y_test_final, y_test_pred, average=None)\n",
        "for i, rec_class in enumerate(recall_per_class):\n",
        "    print(f\"{le.classes_[i]:<20}: {rec_class:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm_test = confusion_matrix(y_test_final, y_test_pred, normalize='true')\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_test, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title('Test Set Confusion Matrix (LightGBM + Focal Loss)', fontsize=16)\n",
        "plt.xlabel('Predicted', fontsize=14)\n",
        "plt.ylabel('True', fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_lightgbm.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importance = final_model.feature_importance(importance_type='gain')\n",
        "feature_names = [f'f{i}' for i in range(len(feature_importance))]\n",
        "\n",
        "# Get top 20 features\n",
        "top_indices = np.argsort(feature_importance)[::-1][:20]\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"{i+1:2d}. {feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(20), feature_importance[top_indices][::-1])\n",
        "plt.yticks(range(20), [feature_names[i] for i in top_indices[::-1]])\n",
        "plt.xlabel('Feature Importance (Gain)')\n",
        "plt.title('Top 20 Feature Importance (LightGBM + Focal Loss)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_focal_loss.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save models and preprocessors\n",
        "save_dir = '/content/drive/MyDrive/models'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save model\n",
        "model_file = os.path.join(save_dir, 'lightgbm_focal_loss_model.txt')\n",
        "final_model.save_model(model_file)\n",
        "print(f\"\\nModel saved to {model_file}\")\n",
        "\n",
        "# Save preprocessors\n",
        "scaler_file = os.path.join(save_dir, 'focal_loss_scaler.joblib')\n",
        "joblib.dump(scaler, scaler_file)\n",
        "print(f\"Scaler saved to {scaler_file}\")\n",
        "\n",
        "encoder_file = os.path.join(save_dir, 'focal_loss_label_encoder.joblib')\n",
        "joblib.dump(le, encoder_file)\n",
        "print(f\"Label encoder saved to {encoder_file}\")\n",
        "\n",
        "tfidf_file = os.path.join(save_dir, 'focal_loss_tfidf.joblib')\n",
        "joblib.dump(tfidf, tfidf_file)\n",
        "print(f\"TF-IDF vectorizer saved to {tfidf_file}\")\n",
        "\n",
        "# Save focal loss trainer and parameters\n",
        "focal_trainer_file = os.path.join(save_dir, 'focal_loss_trainer.joblib')\n",
        "joblib.dump(focal_loss_trainer, focal_trainer_file)\n",
        "print(f\"Focal loss trainer saved to {focal_trainer_file}\")\n",
        "\n",
        "# Save class weights\n",
        "class_weights_file = os.path.join(save_dir, 'focal_loss_class_weights.joblib')\n",
        "joblib.dump(class_weights, class_weights_file)\n",
        "print(f\"Class weights saved to {class_weights_file}\")\n",
        "\n",
        "# Performance comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Performance Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(\"Original XGBoost (with class weights):\")\n",
        "print(f\"  - Test Accuracy: 0.7946\")\n",
        "print(f\"  - Test F1-Score: 0.7953\")\n",
        "print(f\"  - Test Log Loss: 0.6202\")\n",
        "print(f\"\\nLightGBM + Focal Loss:\")\n",
        "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"  - Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"  - Test Log Loss: {test_loss:.4f}\")\n",
        "print(f\"  - Test Focal Loss: {test_focal_loss:.4f}\")\n",
        "\n",
        "improvement_acc = (test_accuracy - 0.7946) / 0.7946 * 100\n",
        "improvement_f1 = (test_f1 - 0.7953) / 0.7953 * 100\n",
        "improvement_loss = (0.6202 - test_loss) / 0.6202 * 100\n",
        "\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  - Accuracy: {improvement_acc:+.2f}%\")\n",
        "print(f\"  - F1-Score: {improvement_f1:+.2f}%\")\n",
        "print(f\"  - Log Loss: {improvement_loss:+.2f}%\")\n",
        "\n",
        "# Focal loss analysis\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Focal Loss Analysis:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\u2022 Alpha (class weight factor): {focal_loss_trainer.alpha}\")\n",
        "print(f\"\u2022 Gamma (focusing parameter): {focal_loss_trainer.gamma}\")\n",
        "print(f\"\u2022 Iterative reweighting: 3 iterations\")\n",
        "print(f\"\u2022 Higher gamma = more focus on hard examples\")\n",
        "print(f\"\u2022 Focal loss helps with class imbalance by down-weighting easy examples\")\n",
        "print(f\"\u2022 Combined with class weights for even better imbalance handling\")\n",
        "\n",
        "# Function to make predictions with the saved model\n",
        "def predict_with_focal_loss(model_path, scaler_path, tfidf_path, encoder_path, focal_trainer_path,\n",
        "                           new_text, new_embeddings):\n",
        "    \"\"\"\n",
        "    Function to make predictions using the saved LightGBM + Focal Loss model\n",
        "    \"\"\"\n",
        "    # Load model and preprocessors\n",
        "    model = lgb.Booster(model_file=model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    tfidf = joblib.load(tfidf_path)\n",
        "    le = joblib.load(encoder_path)\n",
        "    focal_trainer = joblib.load(focal_trainer_path)\n",
        "\n",
        "    # Preprocess new data\n",
        "    tfidf_features = tfidf.transform(new_text).toarray()\n",
        "    features = np.hstack([new_embeddings, tfidf_features])\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    # Make predictions\n",
        "    probabilities = model.predict(features_scaled)\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "    predicted_labels = le.inverse_transform(predictions)\n",
        "\n",
        "    # Calculate focal loss for the predictions (optional)\n",
        "    focal_loss = focal_trainer.calculate_focal_loss(predictions, probabilities)\n",
        "\n",
        "    return predicted_labels, probabilities, focal_loss\n",
        "\n",
        "print(f\"\\nPrediction function created. Use predict_with_lightgbm() for new predictions.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}