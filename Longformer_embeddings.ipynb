{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12405328,
          "sourceType": "datasetVersion",
          "datasetId": 7823175
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\nfrom torch.utils.data import Dataset, DataLoader\n\n# Reset index after dropping NA values\ndata = pd.read_csv('/kaggle/input/combined-data-balanced/combined_data_balanced.csv')\nndata = data.dropna().reset_index(drop=True)  # Reset index here\n\ntext = ndata['text']\nlabels = ndata['mental_state'].values \nlabel_map = {emotion: idx for idx, emotion in enumerate(np.unique(labels))}\nnum_labels = len(label_map)\ny_encoded = np.array([label_map[l] for l in labels])",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:26:12.152883Z",
          "iopub.execute_input": "2025-07-08T04:26:12.153124Z",
          "iopub.status.idle": "2025-07-08T04:26:38.583585Z",
          "shell.execute_reply.started": "2025-07-08T04:26:12.153104Z",
          "shell.execute_reply": "2025-07-08T04:26:38.582771Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-07-08 04:26:25.196191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751948785.378074      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751948785.432872      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    text, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T05:09:38.882604Z",
          "iopub.execute_input": "2025-07-08T05:09:38.883383Z",
          "iopub.status.idle": "2025-07-08T05:09:38.897754Z",
          "shell.execute_reply.started": "2025-07-08T05:09:38.883359Z",
          "shell.execute_reply": "2025-07-08T05:09:38.897148Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "from tqdm import tqdm\nMAX_LEN  = 4095\n# STEP 2: PRE-TOKENIZE DATA (Replace EmotionDataset)\ntokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\ndef pre_tokenize(texts, labels, tokenizer, max_len):\n    input_ids = []\n    attention_masks = []\n    for text in tqdm(texts):\n        enc = tokenizer.encode_plus(\n            text,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True\n        )\n        input_ids.append(enc['input_ids'])\n        attention_masks.append(enc['attention_mask'])\n    return {\n        'input_ids': torch.tensor(input_ids),\n        'attention_mask': torch.tensor(attention_masks),\n        'labels': torch.tensor(labels)\n    }\n\n# Apply to train/val\ntrain_data = pre_tokenize(X_train.tolist(), y_train, tokenizer, MAX_LEN)\nval_data = pre_tokenize(X_val.tolist(), y_val, tokenizer, MAX_LEN)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T05:10:11.206021Z",
          "iopub.execute_input": "2025-07-08T05:10:11.206552Z",
          "iopub.status.idle": "2025-07-08T05:10:34.177954Z",
          "shell.execute_reply.started": "2025-07-08T05:10:11.206529Z",
          "shell.execute_reply": "2025-07-08T05:10:34.177184Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6892/6892 [00:09<00:00, 728.67it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1724/1724 [00:02<00:00, 755.12it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "from transformers import LongformerForSequenceClassification,LongformerTokenizer\n# Load model\nmodel = LongformerForSequenceClassification.from_pretrained(\n    'allenai/longformer-base-4096',\n    num_labels=num_labels\n).to(device)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T05:15:44.468312Z",
          "iopub.execute_input": "2025-07-08T05:15:44.468887Z",
          "iopub.status.idle": "2025-07-08T05:15:45.700927Z",
          "shell.execute_reply.started": "2025-07-08T05:15:44.468866Z",
          "shell.execute_reply": "2025-07-08T05:15:45.700367Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "!pip install transformers lightgbm sentencepiece\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import LongformerTokenizer, LongformerModel\nimport torch\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport time\nfrom sklearn.metrics import classification_report\n\n# Load your dataset\n# df = pd.read_csv('your_data.csv')  # Should have 'text' and 'label' columns\n\n# Initialize Longformer (handles up to 4096 tokens)\ntokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nmodel = LongformerModel.from_pretrained('allenai/longformer-base-4096')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nmodel.eval()\n\n# Truncation function (Longformer max is 4096 tokens)\ndef truncate_text(text):\n    tokens = tokenizer.tokenize(text)\n    return tokenizer.convert_tokens_to_string(tokens[:4095])  # 4096 - [CLS] token\n\n# Embedding generation\ndef get_longformer_embeddings(texts, batch_size=8):\n    embeddings = []\n    \n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch = texts[i:i+batch_size].tolist()\n        \n        # Truncate long texts\n        batch = [truncate_text(text) for text in batch]\n        \n        inputs = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=4096,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        ).to(device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        # Attention-weighted mean pooling\n        attention_mask = inputs['attention_mask']\n        last_hidden = outputs.last_hidden_state\n        embeddings_batch = (last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1).unsqueeze(-1)\n        embeddings.append(embeddings_batch.cpu().numpy())\n    \n    return np.vstack(embeddings)\n\n# Preprocess data\nprint(\"Preprocessing texts...\")\nndata['text'] = ndata['text'].apply(lambda x: x[:100000] if len(x) > 100000 else x)  # Limit extreme lengths\n\n# Generate embeddings\nprint(\"Generating Longformer embeddings...\")\nstart_time = time.time()\nembeddings = get_longformer_embeddings(ndata['text'])\nprint(f\"Embeddings generated in {time.time()-start_time:.2f} seconds\")\n\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:36:07.824539Z",
          "iopub.execute_input": "2025-07-08T04:36:07.825278Z",
          "iopub.status.idle": "2025-07-08T04:41:53.118805Z",
          "shell.execute_reply.started": "2025-07-08T04:36:07.825251Z",
          "shell.execute_reply": "2025-07-08T04:41:53.117914Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nPreprocessing texts...\nGenerating Longformer embeddings...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1077/1077 [05:40<00:00,  3.16it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Embeddings generated in 340.55 seconds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(ndata['mental_state'])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    embeddings, y_encoded, test_size=0.2, random_state=42\n)\n\n  \n\ndef train_evaluate_xgboost():\n    from xgboost import XGBClassifier\n    \n    model = XGBClassifier(\n        objective='multi:softmax',\n        num_class=len(le.classes_),\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        eval_metric='mlogloss',\n        early_stopping_rounds=20,\n        use_label_encoder=False,\n        reg_alpha= 0.8,   # L1 regularization (alpha)\n        reg_lambda= 0.5,  # L2 regularization (lambda)\n        gamma= 0.1,       # Minimum loss reduction for split\n\n        # Additional tree parameters\n        min_child_weight= 3,\n        max_delta_step= 2\n    )\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False\n    )\n    \n    y_pred = model.predict(X_test)\n    return model, y_pred\n\n\n\nprint(\"\\nTraining XGBoost...\")\nxgb_model, xgb_pred = train_evaluate_xgboost()\nprint(\"XGBoost Classification Report:\")\nprint(classification_report(y_test, xgb_pred, target_names=le.classes_))\n\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:59:13.359447Z",
          "iopub.execute_input": "2025-07-08T04:59:13.360036Z",
          "iopub.status.idle": "2025-07-08T05:04:45.541332Z",
          "shell.execute_reply.started": "2025-07-08T04:59:13.360012Z",
          "shell.execute_reply": "2025-07-08T05:04:45.540534Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTraining XGBoost...\nXGBoost Classification Report:\n                      precision    recall  f1-score   support\n\n             anxiety       0.62      0.66      0.64       227\n             bipolar       0.61      0.54      0.57       229\n          depression       0.56      0.52      0.54       213\n              lonely       0.67      0.77      0.72       200\n              normal       0.86      0.91      0.88       227\npersonality_disorder       0.61      0.60      0.60       210\n                ptsd       0.70      0.67      0.69       209\n              stress       0.63      0.63      0.63       209\n\n            accuracy                           0.66      1724\n           macro avg       0.66      0.66      0.66      1724\n        weighted avg       0.66      0.66      0.66      1724\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}