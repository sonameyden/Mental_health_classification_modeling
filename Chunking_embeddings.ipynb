{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bDnBtPBKom1",
        "outputId": "0dce967f-6cfc-4199-f4ce-ffb55c21dd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "6455105eee434694ad62051b5988c19f",
            "bb29984ee08d48289de4bd00aca494e4",
            "927f7101869e4a32a5c0fca6ac367496",
            "2aab900aceb9416a8242f211f812d1e3",
            "4172a2d1951d44408946dc02dad5788e",
            "7a5cd164fd534e02b58ce5a637325351",
            "61a56578ec2645c4bbdf597f3b9088ff",
            "1c15d3dcae4f41e698b9a620e9f06d11",
            "786508705eb548e0a8ba65d6124c1db1",
            "19b226d992ff4bea80ca36ffe6f9d427",
            "ebe99e3473af4c7cb1132968a1f5d442",
            "526a54f52b294256b0e92173d41c089c",
            "e09d027e7c804fd6a37e6166f00a18fa",
            "a2834e2e50c749bcb53d0609d85d3ab1",
            "d37ad3f9de544b4a9dd220f02dccef6e",
            "d8cf576115ee4515ab8d1276bb6e296f",
            "cc77eeb782724537aab9f6a475f81dd0",
            "999928e7c5fd4f33b718a7a0e1b16886",
            "6c8f88b1e32840d8a21410c24ceb4df5",
            "a3751fd435534229b1f44a465c9ed5f4",
            "40de38eec5aa43229f970dabcb60d4c5",
            "eac0953f5c234375b90e68b89d812b31",
            "c1f2a07905444039b4b0980de597c4b5",
            "c3e02d33db11492b8ebc7c9032744043",
            "6e0e84fd72f641519c9d50376bca3710",
            "e2f4d964f1c846ee806e0fae704b92bb",
            "e6d5493df71c480abf7226c30ddc9e21",
            "e6295f89d5844a80ab562a8e3da341d3",
            "254d0205a54f4249a0ad64c02810ea74",
            "b2ce6884c10c47ca88a22cecd1381b34",
            "1fee2f772d764651bfc08eaa6a39e051",
            "48d95cdf6df043b69ed62b4472896cfe",
            "8a0f5fba484841c9bf03631f5e48d834",
            "7c257899de684105aa4116a98016945a",
            "4b83d8b1dde548378946b62671ee56b3",
            "f09624ec23ed460bb10cb885967b031c",
            "86b2fe5e811e42998039995ac56e86e8",
            "f2301bca6549466e92d2ebf1937a6047",
            "8fb7e9ba8b744a4fa59d8fea341cdcaf",
            "f9f1533264e8496fb97fe9a9c4cdaa65",
            "cd3b71d0a87047c09a3d20a2d3cab6c5",
            "c139dda1a09548cbae94f677481c0580",
            "1f82563f165c4253aff5e4be7f87e37d",
            "06db4b51eba94d2ebef24556851c75f9",
            "2232c5ab04e7494aa795001597498799",
            "fe033733f51344fca0e17676d44e5072",
            "31c604c702574cc7acc96911d50c2276",
            "8996dd7566524cb7b1c7bbbeec86364b",
            "f0d38fdf5f37483a99cbb8eb82055e59",
            "0639ad06cb564b28b6aec059fe9aec2d",
            "8c225d4207144b7388128a2245350a8d",
            "5df9cf5e852040c58f7fdcbc36ab37bf",
            "58008ca6de834b3aa728ba2bf19991c2",
            "b622e1cee4474b44ad713d34a7957999",
            "27020a3f5076477ca3e44bae4ab7b34d"
          ]
        },
        "id": "9dr_XHUbDfz2",
        "outputId": "890870e5-89f7-41f6-97ed-f7ded91786d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcc4 Loading data...\n",
            "\n",
            "\ud83e\udde0 Starting coherent augmentation for minority classes...\n",
            "\ud83d\ude80 Loading model: Vamsi/T5_Paraphrase_Paws\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6455105eee434694ad62051b5988c19f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "526a54f52b294256b0e92173d41c089c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1f2a07905444039b4b0980de597c4b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c257899de684105aa4116a98016945a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2232c5ab04e7494aa795001597498799",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Model loaded on CUDA\n",
            "\ud83d\udd0d Minority classes: bipolar, lonely, stress, ptsd, personality disorder\n",
            "\n",
            "\u2699\ufe0f Augmenting 'bipolar' (7765 \u2192 10000 samples)\n",
            "\ud83d\udcdd Generating 2235 augmented samples with coherence preservation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rAugmenting bipolar:   0%|          | 0/7765 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Augmenting bipolar:   0%|          | 32/7765 [01:33<4:36:22,  2.14s/it]"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "class LongTextParaphraser:\n",
        "    def __init__(self, model_name=\"Vamsi/T5_Paraphrase_Paws\", device=None):\n",
        "        print(f\"\ud83d\ude80 Loading model: {model_name}\")\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        print(f\"\u2705 Model loaded on {self.device.upper()}\")\n",
        "\n",
        "    def smart_chunk_text(self, text, max_chunk_tokens=450, overlap_sentences=1):\n",
        "        \"\"\"\n",
        "        Smart chunking that preserves semantic coherence and flow\n",
        "        - Uses paragraph boundaries as primary split points\n",
        "        - Maintains topic coherence within chunks\n",
        "        - Adds overlap between chunks to preserve context\n",
        "        - Falls back to sentence boundaries only when necessary\n",
        "        \"\"\"\n",
        "        # First, try to split by paragraphs (double newlines or clear paragraph markers)\n",
        "        paragraphs = re.split(r'\\n\\s*\\n|\\r\\n\\s*\\r\\n', text.strip())\n",
        "        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "        if len(paragraphs) <= 1:\n",
        "            # No clear paragraphs, use sentence-based chunking with overlap\n",
        "            return self._sentence_chunk_with_overlap(text, max_chunk_tokens, overlap_sentences)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            para_tokens = len(self.tokenizer.tokenize(paragraph))\n",
        "\n",
        "            # If single paragraph is too long, split it intelligently\n",
        "            if para_tokens > max_chunk_tokens:\n",
        "                if current_chunk:\n",
        "                    chunks.append(self._join_with_context(current_chunk))\n",
        "                    current_chunk = []\n",
        "                    current_tokens = 0\n",
        "\n",
        "                # Split long paragraph while preserving semantic units\n",
        "                para_chunks = self._split_long_paragraph(paragraph, max_chunk_tokens, overlap_sentences)\n",
        "                chunks.extend(para_chunks)\n",
        "                continue\n",
        "\n",
        "            # Check if adding this paragraph exceeds limit\n",
        "            if current_tokens + para_tokens > max_chunk_tokens and current_chunk:\n",
        "                chunks.append(self._join_with_context(current_chunk))\n",
        "                current_chunk = [paragraph]\n",
        "                current_tokens = para_tokens\n",
        "            else:\n",
        "                current_chunk.append(paragraph)\n",
        "                current_tokens += para_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(self._join_with_context(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _split_long_paragraph(self, paragraph, max_tokens, overlap_sentences):\n",
        "        \"\"\"Split a long paragraph while maintaining semantic coherence\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', paragraph)\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_tokens = len(self.tokenizer.tokenize(sentence))\n",
        "\n",
        "            if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
        "                # Add overlap from previous chunk\n",
        "                overlap_start = max(0, len(current_chunk) - overlap_sentences)\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "                # Start new chunk with overlap\n",
        "                overlap_sentences_list = current_chunk[overlap_start:] if overlap_sentences > 0 else []\n",
        "                current_chunk = overlap_sentences_list + [sentence]\n",
        "                current_tokens = sum(len(self.tokenizer.tokenize(s)) for s in current_chunk)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_tokens += sentence_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _sentence_chunk_with_overlap(self, text, max_tokens, overlap_sentences):\n",
        "        \"\"\"Fallback sentence-based chunking with overlap for coherence\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(self.tokenizer.tokenize(sentence))\n",
        "\n",
        "            if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "                # Add overlap for context continuity\n",
        "                overlap_start = max(0, len(current_chunk) - overlap_sentences)\n",
        "                overlap_sentences_list = current_chunk[overlap_start:] if overlap_sentences > 0 else []\n",
        "                current_chunk = overlap_sentences_list + [sentence]\n",
        "                current_tokens = sum(len(self.tokenizer.tokenize(s)) for s in current_chunk)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_tokens += sentence_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _join_with_context(self, paragraphs):\n",
        "        \"\"\"Join paragraphs while preserving formatting context\"\"\"\n",
        "        return '\\n\\n'.join(paragraphs)\n",
        "\n",
        "    def paraphrase(self, text, num_return_sequences=1, max_length=512, preserve_structure=True):\n",
        "        \"\"\"\n",
        "        Enhanced paraphrase method with coherence preservation\n",
        "        - preserve_structure: maintains paragraph breaks and overall flow\n",
        "        \"\"\"\n",
        "        if not text.strip():\n",
        "            return [text] * num_return_sequences\n",
        "\n",
        "        token_count = len(self.tokenizer.tokenize(text))\n",
        "\n",
        "        # For short texts, paraphrase directly\n",
        "        if token_count <= 450:\n",
        "            return self._paraphrase_chunk(text, num_return_sequences, max_length)\n",
        "\n",
        "        # For long texts, use smart chunking\n",
        "        chunks = self.smart_chunk_text(text, max_chunk_tokens=450, overlap_sentences=2)\n",
        "\n",
        "        if len(chunks) == 1:\n",
        "            # Text fits in one chunk after smart processing\n",
        "            return self._paraphrase_chunk(chunks[0], num_return_sequences, max_length)\n",
        "\n",
        "        # Process multiple chunks with coherence preservation\n",
        "        return self._paraphrase_long_text(chunks, num_return_sequences, max_length, preserve_structure)\n",
        "\n",
        "    def _paraphrase_long_text(self, chunks, num_return_sequences, max_length, preserve_structure):\n",
        "        \"\"\"Paraphrase long text while maintaining coherence across chunks\"\"\"\n",
        "        all_results = []\n",
        "\n",
        "        for seq_idx in range(num_return_sequences):\n",
        "            paraphrased_chunks = []\n",
        "            previous_context = \"\"\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                # Add context from previous chunk for better coherence\n",
        "                if chunk_idx > 0 and previous_context:\n",
        "                    # Use last sentence of previous chunk as context\n",
        "                    context_prompt = f\"paraphrase: {previous_context} {chunk} </s>\"\n",
        "                else:\n",
        "                    context_prompt = f\"paraphrase: {chunk} </s>\"\n",
        "\n",
        "                chunk_result = self._paraphrase_chunk_with_prompt(\n",
        "                    context_prompt, 1, max_length\n",
        "                )[0]\n",
        "\n",
        "                # Remove potential repetition from context\n",
        "                if chunk_idx > 0 and previous_context:\n",
        "                    chunk_result = self._remove_context_overlap(chunk_result, previous_context)\n",
        "\n",
        "                paraphrased_chunks.append(chunk_result)\n",
        "\n",
        "                # Store context for next chunk (last sentence)\n",
        "                sentences = re.split(r'(?<=[.!?])\\s+', chunk_result.strip())\n",
        "                previous_context = sentences[-1] if sentences else \"\"\n",
        "\n",
        "            # Join chunks appropriately\n",
        "            if preserve_structure:\n",
        "                final_text = self._smart_join_chunks(paraphrased_chunks, chunks)\n",
        "            else:\n",
        "                final_text = ' '.join(paraphrased_chunks)\n",
        "\n",
        "            all_results.append(final_text)\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def _paraphrase_chunk_with_prompt(self, prompt, num_return_sequences, max_length):\n",
        "        \"\"\"Paraphrase with custom prompt (for context-aware processing)\"\"\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            prompt,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=encoding[\"input_ids\"],\n",
        "                attention_mask=encoding[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                do_sample=True,\n",
        "                top_k=120,\n",
        "                top_p=0.95,\n",
        "                early_stopping=True,\n",
        "                temperature=0.7,\n",
        "                repetition_penalty=1.1  # Reduce repetition\n",
        "            )\n",
        "\n",
        "        return [\n",
        "            self.tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for out in outputs\n",
        "        ]\n",
        "\n",
        "    def _paraphrase_chunk(self, text, num_return_sequences, max_length):\n",
        "        \"\"\"Standard chunk paraphrasing\"\"\"\n",
        "        prompt = f\"paraphrase: {text.strip().replace(chr(10), ' ')} </s>\"\n",
        "        return self._paraphrase_chunk_with_prompt(prompt, num_return_sequences, max_length)\n",
        "\n",
        "    def _remove_context_overlap(self, text, context):\n",
        "        \"\"\"Remove potential overlap from context at the beginning of text\"\"\"\n",
        "        context_words = context.lower().split()[-5:]  # Last 5 words of context\n",
        "        text_words = text.lower().split()\n",
        "\n",
        "        # Find overlap\n",
        "        for i in range(min(len(context_words), len(text_words))):\n",
        "            if context_words[-i-1:] == text_words[:i+1]:\n",
        "                # Remove overlap\n",
        "                return ' '.join(text.split()[i+1:])\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _smart_join_chunks(self, paraphrased_chunks, original_chunks):\n",
        "        \"\"\"Join chunks while preserving original structure cues\"\"\"\n",
        "        result = []\n",
        "\n",
        "        for i, (para_chunk, orig_chunk) in enumerate(zip(paraphrased_chunks, original_chunks)):\n",
        "            # Preserve paragraph breaks from original\n",
        "            if '\\n\\n' in orig_chunk:\n",
        "                # Split and rejoin with paragraph breaks\n",
        "                para_parts = para_chunk.split('. ')\n",
        "                if len(para_parts) > 1:\n",
        "                    mid_point = len(para_parts) // 2\n",
        "                    rejoined = '. '.join(para_parts[:mid_point]) + '.\\n\\n' + '. '.join(para_parts[mid_point:])\n",
        "                    result.append(rejoined)\n",
        "                else:\n",
        "                    result.append(para_chunk)\n",
        "            else:\n",
        "                result.append(para_chunk)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# Enhanced augmentation function\n",
        "def augment_minority_class(df, label_column='mental_state', text_column='text',\n",
        "                          target_per_class=10000, min_samples=500):\n",
        "    paraphraser = LongTextParaphraser()\n",
        "    augmented_data = []\n",
        "\n",
        "    class_counts = df[label_column].value_counts()\n",
        "    minority_classes = class_counts[class_counts < target_per_class].index.tolist()\n",
        "\n",
        "    print(f\"\ud83d\udd0d Minority classes: {', '.join(minority_classes)}\")\n",
        "\n",
        "    for label in minority_classes:\n",
        "        class_df = df[df[label_column] == label]\n",
        "        current_count = len(class_df)\n",
        "        needed = max(target_per_class - current_count, min_samples)\n",
        "\n",
        "        print(f\"\\n\u2699\ufe0f Augmenting '{label}' ({current_count} \u2192 {current_count + needed} samples)\")\n",
        "        print(f\"\ud83d\udcdd Generating {needed} augmented samples with coherence preservation...\")\n",
        "\n",
        "        # Create augmentation plan\n",
        "        augmentation_plan = []\n",
        "        base_samples = class_df[text_column].tolist()\n",
        "\n",
        "        # Calculate how many augmentations per original sample\n",
        "        per_sample = max(1, needed // current_count)\n",
        "        remainder = needed % current_count\n",
        "\n",
        "        for i, text in enumerate(base_samples):\n",
        "            count = per_sample + (1 if i < remainder else 0)\n",
        "            augmentation_plan.append((text, count))\n",
        "\n",
        "        # Process with progress bar\n",
        "        for text, count in tqdm(augmentation_plan, desc=f\"Augmenting {label}\"):\n",
        "            try:\n",
        "                # Use enhanced paraphrasing with structure preservation\n",
        "                paraphrases = paraphraser.paraphrase(\n",
        "                    text,\n",
        "                    num_return_sequences=count,\n",
        "                    max_length=512,\n",
        "                    preserve_structure=True\n",
        "                )\n",
        "\n",
        "                for paraphrase in paraphrases:\n",
        "                    augmented_data.append({\n",
        "                        text_column: paraphrase,\n",
        "                        label_column: label,\n",
        "                        'source': 'augmented_coherent'\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"\u274c Error augmenting text: {str(e)[:100]}...\")\n",
        "                # Fallback to original text if needed\n",
        "                for _ in range(count):\n",
        "                    augmented_data.append({\n",
        "                        text_column: text,\n",
        "                        label_column: label,\n",
        "                        'source': 'original_fallback'\n",
        "                    })\n",
        "\n",
        "    # Create augmented DataFrame\n",
        "    augmented_df = pd.DataFrame(augmented_data)\n",
        "\n",
        "    # Combine with original data\n",
        "    original_df = df.copy()\n",
        "    original_df['source'] = 'original'\n",
        "    combined_df = pd.concat([original_df, augmented_df], ignore_index=True)\n",
        "\n",
        "    print(f\"\\n\u2705 Coherent augmentation complete! Final counts:\")\n",
        "    print(combined_df[label_column].value_counts())\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83d\udcc4 Loading data...\")\n",
        "    data = pd.read_csv(\"/content/drive/My Drive/combined_data.csv\")\n",
        "    df = data.dropna().reset_index(drop=True)\n",
        "    df = df[[\"text\", \"mental_state\"]]\n",
        "\n",
        "    print(\"\\n\ud83e\udde0 Starting coherent augmentation for minority classes...\")\n",
        "    df_augmented = augment_minority_class(\n",
        "        df,\n",
        "        target_per_class=10000,\n",
        "        min_samples=1000\n",
        "    )\n",
        "\n",
        "    output_path = \"/content/drive/My Drive/augmented_data_coherent_t5_yy.csv\"\n",
        "    df_augmented.to_csv(output_path, index=False)\n",
        "    print(f\"\\n\ud83d\udcbe Saved coherently augmented dataset to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in directory: ['6epeochs.ipynb', 'chunking_augmentaion.ipynb', 'combined_data.csv', 'embeding_different_cleaned_text.ipynb', 'enhanced_bert_results_will_it_work', 'enhanced_bert_results_will_it_work-20250717T020940Z-1-001.zip', 'focalloss-LGBM_classifier.ipynb', 'focalloss_LBGM (1).ipynb', 'FocalLoss_LBGM_Results.ipynb', 'FocalLoss_weighted_classifier.ipynb', 'improvedd-1 (2).ipynb', 'longformer-lightgm.ipynb', 'Longformer_embeddings.ipynb', 'Roberta_embeddings.ipynb', 'roberta_end_end_tried.ipynb', 'Roberta_LightGM.ipynb', 'simple_augmentation.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "folder_path = r'C:\\Users\\ADMIN\\Desktop\\Github_upload\\mental_health'\n",
        "print(\"Files in directory:\", os.listdir(folder_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Use raw strings (r'...') to avoid unicode escape errors\n",
        "input_path = r'C:\\Users\\ADMIN\\Desktop\\Github_upload\\mental_health\\chunking_augmentaion.ipynb'\n",
        "output_path = r'C:\\Users\\ADMIN\\Desktop\\Github_upload\\mental_health\\Chunking_embddings.ipynb'\n",
        "\n",
        "# Load the notebook\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    notebook = json.load(f)\n",
        "\n",
        "# Remove 'widgets' metadata if present\n",
        "if 'widgets' in notebook.get('metadata', {}):\n",
        "    del notebook['metadata']['widgets']\n",
        "\n",
        "# Remove 'widgets' from each cell's metadata\n",
        "for cell in notebook.get('cells', []):\n",
        "    if 'widgets' in cell.get('metadata', {}):\n",
        "        del cell['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(notebook, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}