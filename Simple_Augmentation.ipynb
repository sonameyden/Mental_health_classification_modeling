{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MjM7rFdGJuiR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bDnBtPBKom1",
        "outputId": "233c3e1e-1075-420f-8ce7-06739df22ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "from sklearn.utils import resample\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "except:\n",
        "    print(\"NLTK data download failed - some features may not work\")\n",
        "\n",
        "class MentalHealthTextAugmenter:\n",
        "    def __init__(self, target_samples_per_class=15000, min_words=500, max_words=15000):\n",
        "        \"\"\"\n",
        "        Advanced text augmentation for mental health classification\n",
        "\n",
        "        Args:\n",
        "            target_samples_per_class: Target number of samples per class after augmentation\n",
        "            min_words: Minimum word count for augmented texts\n",
        "            max_words: Maximum word count for augmented texts\n",
        "        \"\"\"\n",
        "        self.target_samples_per_class = target_samples_per_class\n",
        "        self.min_words = min_words\n",
        "        self.max_words = max_words\n",
        "\n",
        "        # Initialize models (lazy loading)\n",
        "        self.paraphrase_model = None\n",
        "        self.sentence_model = None\n",
        "\n",
        "        # Mental health domain-specific synonyms and patterns\n",
        "        self.domain_synonyms = {\n",
        "            'anxiety': ['worry', 'nervousness', 'unease', 'apprehension', 'concern', 'fear'],\n",
        "            'depression': ['sadness', 'melancholy', 'despair', 'gloom', 'dejection'],\n",
        "            'stress': ['pressure', 'strain', 'tension', 'burden', 'overwhelm'],\n",
        "            'lonely': ['isolated', 'alone', 'solitary', 'disconnected', 'friendless'],\n",
        "            'suicidal': ['hopeless', 'desperate', 'helpless', 'lost', 'trapped'],\n",
        "            'bipolar': ['mood swings', 'emotional instability', 'ups and downs'],\n",
        "            'ptsd': ['trauma', 'flashbacks', 'nightmares', 'triggers'],\n",
        "            'feel': ['experience', 'sense', 'perceive', 'undergo', 'encounter'],\n",
        "            'think': ['believe', 'consider', 'contemplate', 'reflect', 'ponder'],\n",
        "            'help': ['support', 'assistance', 'aid', 'guidance', 'relief']\n",
        "        }\n",
        "\n",
        "    def _init_models(self):\n",
        "        \"\"\"Initialize heavy models only when needed\"\"\"\n",
        "        if self.paraphrase_model is None:\n",
        "            try:\n",
        "                # Use a lightweight paraphrasing approach\n",
        "                self.paraphrase_model = pipeline(\n",
        "                    \"text2text-generation\",\n",
        "                    model=\"t5-small\",\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "            except:\n",
        "                print(\"Warning: Paraphrase model failed to load, using synonym replacement only\")\n",
        "                self.paraphrase_model = \"failed\"\n",
        "\n",
        "        if self.sentence_model is None:\n",
        "            try:\n",
        "                self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            except:\n",
        "                print(\"Warning: Sentence model failed to load\")\n",
        "                self.sentence_model = \"failed\"\n",
        "\n",
        "    def get_synonyms(self, word: str, pos: str = None) -> List[str]:\n",
        "        \"\"\"Get synonyms for a word using WordNet and domain knowledge\"\"\"\n",
        "        synonyms = set()\n",
        "\n",
        "        # Check domain-specific synonyms first\n",
        "        word_lower = word.lower()\n",
        "        for key, values in self.domain_synonyms.items():\n",
        "            if word_lower == key or word_lower in values:\n",
        "                synonyms.update(values)\n",
        "\n",
        "        # Use WordNet for additional synonyms\n",
        "        try:\n",
        "            for syn in wordnet.synsets(word):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonym = lemma.name().replace('_', ' ')\n",
        "                    if synonym.lower() != word.lower():\n",
        "                        synonyms.add(synonym)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return list(synonyms)[:3]  # Limit to top 3 synonyms\n",
        "\n",
        "    def synonym_replacement(self, text: str, n: int = None) -> str:\n",
        "        \"\"\"Replace n random words with their synonyms\"\"\"\n",
        "        words = text.split()\n",
        "        if n is None:\n",
        "            n = max(1, len(words) // 20)  # Replace ~5% of words\n",
        "\n",
        "        new_words = words.copy()\n",
        "        random_word_list = list(set([word for word in words if word.isalpha()]))\n",
        "        random.shuffle(random_word_list)\n",
        "\n",
        "        num_replaced = 0\n",
        "        for random_word in random_word_list:\n",
        "            synonyms = self.get_synonyms(random_word)\n",
        "            if synonyms and num_replaced < n:\n",
        "                synonym = random.choice(synonyms)\n",
        "                new_words = [synonym if word == random_word else word for word in new_words]\n",
        "                num_replaced += 1\n",
        "\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    def random_insertion(self, text: str, n: int = None) -> str:\n",
        "        \"\"\"Randomly insert n synonyms into the sentence\"\"\"\n",
        "        words = text.split()\n",
        "        if n is None:\n",
        "            n = max(1, len(words) // 25)  # Insert ~4% new words\n",
        "\n",
        "        for _ in range(n):\n",
        "            random_word = random.choice([w for w in words if w.isalpha()])\n",
        "            synonyms = self.get_synonyms(random_word)\n",
        "            if synonyms:\n",
        "                random_synonym = random.choice(synonyms)\n",
        "                random_idx = random.randint(0, len(words))\n",
        "                words.insert(random_idx, random_synonym)\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def random_swap(self, text: str, n: int = None) -> str:\n",
        "        \"\"\"Randomly swap two words in the sentence n times\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) < 4:\n",
        "            return text\n",
        "\n",
        "        if n is None:\n",
        "            n = max(1, len(words) // 30)  # Swap ~3% of words\n",
        "\n",
        "        for _ in range(n):\n",
        "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def random_deletion(self, text: str, p: float = 0.05) -> str:\n",
        "        \"\"\"Randomly delete words from the sentence with probability p\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) == 1:\n",
        "            return text\n",
        "\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            if random.uniform(0, 1) > p:\n",
        "                new_words.append(word)\n",
        "\n",
        "        # If all words were deleted, return original\n",
        "        if len(new_words) == 0:\n",
        "            return text\n",
        "\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    def sentence_level_augmentation(self, text: str) -> str:\n",
        "        \"\"\"Apply sentence-level transformations\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        augmented_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # Randomly apply transformations\n",
        "            if random.random() < 0.3:  # 30% chance\n",
        "                # Sentence reordering within paragraphs\n",
        "                words = sentence.split()\n",
        "                if len(words) > 5:\n",
        "                    # Split into chunks and reorder\n",
        "                    mid_point = len(words) // 2\n",
        "                    if random.random() < 0.5:\n",
        "                        sentence = ' '.join(words[mid_point:] + words[:mid_point])\n",
        "\n",
        "            augmented_sentences.append(sentence)\n",
        "\n",
        "        return '. '.join(augmented_sentences)\n",
        "\n",
        "    def length_normalization(self, text: str, target_length: int) -> str:\n",
        "        \"\"\"Normalize text length by expansion or contraction\"\"\"\n",
        "        words = text.split()\n",
        "        current_length = len(words)\n",
        "\n",
        "        if current_length < target_length * 0.8:\n",
        "            # Expand text\n",
        "            expansion_factor = target_length / current_length\n",
        "            if expansion_factor > 1.5:\n",
        "                # Use repetition and elaboration\n",
        "                elaboration_phrases = [\n",
        "                    \"I really feel that\", \"It's important to note that\", \"What I mean is\",\n",
        "                    \"To elaborate further\", \"Additionally\", \"Furthermore\", \"In other words\",\n",
        "                    \"This makes me think\", \"I can't help but feel\", \"It seems to me that\"\n",
        "                ]\n",
        "\n",
        "                sentences = re.split(r'[.!?]+', text)\n",
        "                expanded_sentences = []\n",
        "\n",
        "                for sentence in sentences:\n",
        "                    if sentence.strip():\n",
        "                        expanded_sentences.append(sentence.strip())\n",
        "                        if random.random() < 0.4 and len(expanded_sentences) < target_length // 20:\n",
        "                            phrase = random.choice(elaboration_phrases)\n",
        "                            elaboration = f\"{phrase} {sentence.strip().lower()}\"\n",
        "                            expanded_sentences.append(elaboration)\n",
        "\n",
        "                text = '. '.join(expanded_sentences)\n",
        "\n",
        "        elif current_length > target_length * 1.2:\n",
        "            # Contract text by removing less important sentences\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            # Keep first and last sentences, randomly sample middle ones\n",
        "            if len(sentences) > 3:\n",
        "                keep_ratio = target_length / current_length\n",
        "                middle_sentences = sentences[1:-1]\n",
        "                keep_count = max(1, int(len(middle_sentences) * keep_ratio))\n",
        "                kept_middle = random.sample(middle_sentences, min(keep_count, len(middle_sentences)))\n",
        "                sentences = [sentences[0]] + kept_middle + [sentences[-1]]\n",
        "                text = '. '.join([s.strip() for s in sentences if s.strip()])\n",
        "\n",
        "        return text\n",
        "\n",
        "    def augment_text(self, text: str, augmentation_type: str = 'mixed') -> str:\n",
        "\n",
        "         original_length = len(text.split())\n",
        "\n",
        "    # Calculate bounds\n",
        "         min_len = max(self.min_words, int(original_length * 0.7))\n",
        "         max_len = min(self.max_words, int(original_length * 1.5))\n",
        "\n",
        "    # \u2705 Safe-guard: avoid ValueError when min_len > max_len\n",
        "         if min_len > max_len:\n",
        "\n",
        "           target_length = original_length  # fallback\n",
        "         else:\n",
        "            target_length = random.randint(min_len, max_len)\n",
        "\n",
        "         augmented = text\n",
        "\n",
        "         if augmentation_type in ['mixed', 'eda']:\n",
        "        # Apply EDA techniques\n",
        "            techniques = [\n",
        "            self.synonym_replacement,\n",
        "            self.random_insertion,\n",
        "            self.random_swap,\n",
        "            self.random_deletion\n",
        "        ]\n",
        "            num_techniques = random.randint(1, 2)\n",
        "            selected_techniques = random.sample(techniques, num_techniques)\n",
        "\n",
        "            for technique in selected_techniques:\n",
        "                  if technique == self.random_deletion:\n",
        "                     augmented = technique(augmented, p=0.03)  # Lower deletion rate\n",
        "                  else:\n",
        "                      augmented = technique(augmented)\n",
        "\n",
        "         if augmentation_type in ['mixed', 'sentence']:\n",
        "        # Apply sentence-level transformations\n",
        "              augmented = self.sentence_level_augmentation(augmented)\n",
        "\n",
        "    # Normalize to target length\n",
        "         augmented = self.length_normalization(augmented, target_length)\n",
        "\n",
        "         return augmented\n",
        "\n",
        "\n",
        "    def calculate_augmentation_needs(self, df: pd.DataFrame) -> Dict[str, int]:\n",
        "        \"\"\"Calculate how many samples each class needs\"\"\"\n",
        "        class_counts = df['mental_state'].value_counts()\n",
        "        augmentation_needs = {}\n",
        "\n",
        "        for class_name in class_counts.index:\n",
        "            current_count = class_counts[class_name]\n",
        "            needed = max(0, self.target_samples_per_class - current_count)\n",
        "            augmentation_needs[class_name] = needed\n",
        "\n",
        "        return augmentation_needs\n",
        "\n",
        "    def augment_class(self, class_df: pd.DataFrame, needed_samples: int,\n",
        "                     class_name: str) -> pd.DataFrame:\n",
        "        \"\"\"Augment a specific class\"\"\"\n",
        "        if needed_samples <= 0:\n",
        "            return class_df\n",
        "\n",
        "        print(f\"Augmenting {class_name}: {len(class_df)} -> {len(class_df) + needed_samples}\")\n",
        "\n",
        "        augmented_data = []\n",
        "        original_texts = class_df['text'].tolist()\n",
        "\n",
        "        # Create multiple augmented versions of each text\n",
        "        samples_per_original = max(1, needed_samples // len(original_texts))\n",
        "\n",
        "        for text in original_texts:\n",
        "            for i in range(samples_per_original):\n",
        "                if len(augmented_data) >= needed_samples:\n",
        "                    break\n",
        "\n",
        "                # Use different augmentation strategies\n",
        "                strategies = ['mixed', 'eda', 'sentence']\n",
        "                strategy = random.choice(strategies)\n",
        "\n",
        "                augmented_text = self.augment_text(text, strategy)\n",
        "\n",
        "                # Quality check - ensure minimum difference from original\n",
        "                if len(set(augmented_text.split()) - set(text.split())) > 5:\n",
        "                    augmented_data.append({\n",
        "                        'text': augmented_text,\n",
        "                        'mental_state': class_name,\n",
        "                        'augmented': True\n",
        "                    })\n",
        "\n",
        "            if len(augmented_data) >= needed_samples:\n",
        "                break\n",
        "\n",
        "        # If we still need more samples, repeat with higher variation\n",
        "        while len(augmented_data) < needed_samples:\n",
        "            text = random.choice(original_texts)\n",
        "            augmented_text = self.augment_text(text, 'mixed')\n",
        "            augmented_data.append({\n",
        "                'text': augmented_text,\n",
        "                'mental_state': class_name,\n",
        "                'augmented': True\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(augmented_data[:needed_samples])\n",
        "\n",
        "    def augment_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Augment the entire dataset\"\"\"\n",
        "        print(\"=== Mental Health Text Augmentation ===\")\n",
        "        print(f\"Original dataset shape: {df.shape}\")\n",
        "        print(f\"Target samples per class: {self.target_samples_per_class}\")\n",
        "\n",
        "        # Add augmented flag to original data\n",
        "        df = df.copy()\n",
        "        df['augmented'] = False\n",
        "\n",
        "        # Calculate augmentation needs\n",
        "        augmentation_needs = self.calculate_augmentation_needs(df)\n",
        "\n",
        "        print(\"\\nAugmentation needs:\")\n",
        "        for class_name, needed in augmentation_needs.items():\n",
        "            current = len(df[df['mental_state'] == class_name])\n",
        "            print(f\"  {class_name}: {current} -> {current + needed} (+{needed})\")\n",
        "\n",
        "        # Augment each class\n",
        "        augmented_dfs = [df]\n",
        "\n",
        "        for class_name, needed_samples in augmentation_needs.items():\n",
        "            if needed_samples > 0:\n",
        "                class_df = df[df['mental_state'] == class_name]\n",
        "                augmented_class_df = self.augment_class(class_df, needed_samples, class_name)\n",
        "                augmented_dfs.append(augmented_class_df)\n",
        "\n",
        "        # Combine all data\n",
        "        final_df = pd.concat(augmented_dfs, ignore_index=True)\n",
        "\n",
        "        print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
        "        print(f\"Augmented samples: {final_df['augmented'].sum()}\")\n",
        "\n",
        "        # Show final distribution\n",
        "        print(\"\\nFinal class distribution:\")\n",
        "        final_counts = final_df['mental_state'].value_counts()\n",
        "        for class_name, count in final_counts.items():\n",
        "            percentage = (count / len(final_df)) * 100\n",
        "            print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        return final_df\n",
        "\n",
        "# Usage example and utility functions\n",
        "def load_and_preprocess_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load and preprocess the mental health dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic preprocessing\n",
        "    df = df.dropna(subset=['text', 'mental_state'])\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    df['mental_state'] = df['mental_state'].astype(str)\n",
        "\n",
        "    # Remove very short texts (less than 50 words)\n",
        "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "    df = df[df['word_count'] >= 50].copy()\n",
        "\n",
        "    return df[['text', 'mental_state']]\n",
        "\n",
        "def analyze_text_lengths(df: pd.DataFrame):\n",
        "    \"\"\"Analyze text length distribution by class\"\"\"\n",
        "    print(\"=== Text Length Analysis ===\")\n",
        "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    for class_name in df['mental_state'].unique():\n",
        "        class_texts = df[df['mental_state'] == class_name]['word_count']\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(f\"  Count: {len(class_texts)}\")\n",
        "        print(f\"  Mean length: {class_texts.mean():.0f} words\")\n",
        "        print(f\"  Median length: {class_texts.median():.0f} words\")\n",
        "        print(f\"  Length range: {class_texts.min()}-{class_texts.max()} words\")\n",
        "\n",
        "# Main execution example\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_and_preprocess_data('/content/drive/My Drive/combined_data.csv')\n",
        "\n",
        "    # Analyze original distribution\n",
        "    analyze_text_lengths(df)\n",
        "\n",
        "    # Initialize augmenter with balanced targets\n",
        "    augmenter = MentalHealthTextAugmenter(\n",
        "        target_samples_per_class=12000,  # Adjust based on your needs\n",
        "        min_words=1000,\n",
        "        max_words=12000\n",
        "    )\n",
        "\n",
        "    # Augment the dataset\n",
        "    augmented_df = augmenter.augment_dataset(df)\n",
        "\n",
        "    # Save the augmented dataset\n",
        "    augmented_df.to_csv(\"/content/drive/My Drive/augmented_mental_health_data.csv\", index=False)\n",
        "    print(\"\\nAugmented dataset saved to 'augmented_mental_health_data.csv'\")\n",
        "\n",
        "    # Analyze final distribution\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    analyze_text_lengths(augmented_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-GKvkpBnB7Z",
        "outputId": "8a413d98-3398-4056-8c1f-16dbca0dda36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "=== Text Length Analysis ===\n",
            "\n",
            "anxiety:\n",
            "  Count: 14481\n",
            "  Mean length: 197 words\n",
            "  Median length: 151 words\n",
            "  Length range: 50-3009 words\n",
            "\n",
            "normal:\n",
            "  Count: 1314\n",
            "  Mean length: 85 words\n",
            "  Median length: 79 words\n",
            "  Length range: 50-255 words\n",
            "\n",
            "depression:\n",
            "  Count: 40532\n",
            "  Mean length: 214 words\n",
            "  Median length: 156 words\n",
            "  Length range: 50-4830 words\n",
            "\n",
            "suicidal:\n",
            "  Count: 19335\n",
            "  Mean length: 207 words\n",
            "  Median length: 148 words\n",
            "  Length range: 50-5248 words\n",
            "\n",
            "stress:\n",
            "  Count: 2219\n",
            "  Mean length: 128 words\n",
            "  Median length: 95 words\n",
            "  Length range: 50-1606 words\n",
            "\n",
            "bipolar:\n",
            "  Count: 6800\n",
            "  Mean length: 207 words\n",
            "  Median length: 154 words\n",
            "  Length range: 50-4804 words\n",
            "\n",
            "personality disorder:\n",
            "  Count: 876\n",
            "  Mean length: 214 words\n",
            "  Median length: 168 words\n",
            "  Length range: 50-5419 words\n",
            "\n",
            "lonely:\n",
            "  Count: 2158\n",
            "  Mean length: 187 words\n",
            "  Median length: 140 words\n",
            "  Length range: 50-2078 words\n",
            "\n",
            "ptsd:\n",
            "  Count: 1249\n",
            "  Mean length: 233 words\n",
            "  Median length: 174 words\n",
            "  Length range: 50-4507 words\n",
            "=== Mental Health Text Augmentation ===\n",
            "Original dataset shape: (88964, 3)\n",
            "Target samples per class: 12000\n",
            "\n",
            "Augmentation needs:\n",
            "  depression: 40532 -> 40532 (+0)\n",
            "  suicidal: 19335 -> 19335 (+0)\n",
            "  anxiety: 14481 -> 14481 (+0)\n",
            "  bipolar: 6800 -> 12000 (+5200)\n",
            "  stress: 2219 -> 12000 (+9781)\n",
            "  lonely: 2158 -> 12000 (+9842)\n",
            "  normal: 1314 -> 12000 (+10686)\n",
            "  ptsd: 1249 -> 12000 (+10751)\n",
            "  personality disorder: 876 -> 12000 (+11124)\n",
            "Augmenting bipolar: 6800 -> 12000\n",
            "Augmenting stress: 2219 -> 12000\n",
            "Augmenting lonely: 2158 -> 12000\n",
            "Augmenting normal: 1314 -> 12000\n",
            "Augmenting ptsd: 1249 -> 12000\n",
            "Augmenting personality disorder: 876 -> 12000\n",
            "\n",
            "Final dataset shape: (146348, 4)\n",
            "Augmented samples: 57384\n",
            "\n",
            "Final class distribution:\n",
            "  depression: 40532 (27.7%)\n",
            "  suicidal: 19335 (13.2%)\n",
            "  anxiety: 14481 (9.9%)\n",
            "  normal: 12000 (8.2%)\n",
            "  stress: 12000 (8.2%)\n",
            "  bipolar: 12000 (8.2%)\n",
            "  personality disorder: 12000 (8.2%)\n",
            "  lonely: 12000 (8.2%)\n",
            "  ptsd: 12000 (8.2%)\n",
            "\n",
            "Augmented dataset saved to 'augmented_mental_health_data.csv'\n",
            "\n",
            "==================================================\n",
            "=== Text Length Analysis ===\n",
            "\n",
            "anxiety:\n",
            "  Count: 14481\n",
            "  Mean length: 197 words\n",
            "  Median length: 151 words\n",
            "  Length range: 50-3009 words\n",
            "\n",
            "normal:\n",
            "  Count: 12000\n",
            "  Mean length: 87 words\n",
            "  Median length: 81 words\n",
            "  Length range: 45-269 words\n",
            "\n",
            "depression:\n",
            "  Count: 40532\n",
            "  Mean length: 214 words\n",
            "  Median length: 156 words\n",
            "  Length range: 50-4830 words\n",
            "\n",
            "suicidal:\n",
            "  Count: 19335\n",
            "  Mean length: 207 words\n",
            "  Median length: 148 words\n",
            "  Length range: 50-5248 words\n",
            "\n",
            "stress:\n",
            "  Count: 12000\n",
            "  Mean length: 143 words\n",
            "  Median length: 103 words\n",
            "  Length range: 33-1672 words\n",
            "\n",
            "bipolar:\n",
            "  Count: 12000\n",
            "  Mean length: 227 words\n",
            "  Median length: 171 words\n",
            "  Length range: 48-5155 words\n",
            "\n",
            "personality disorder:\n",
            "  Count: 12000\n",
            "  Mean length: 240 words\n",
            "  Median length: 194 words\n",
            "  Length range: 46-5815 words\n",
            "\n",
            "lonely:\n",
            "  Count: 12000\n",
            "  Mean length: 211 words\n",
            "  Median length: 159 words\n",
            "  Length range: 45-2196 words\n",
            "\n",
            "ptsd:\n",
            "  Count: 12000\n",
            "  Mean length: 267 words\n",
            "  Median length: 199 words\n",
            "  Length range: 46-6067 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gtj1W2h4oJWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}