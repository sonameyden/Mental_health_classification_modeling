{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmdenaAI/Bhutan-Mental-Health/blob/ml_nlp/Model_classification_Tsovinar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQFr4S7gPZpI"
      },
      "source": [
        "##Lets try to get the libraries we need for the data explaratory and modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-09T05:23:06.386288Z",
          "iopub.status.busy": "2025-07-09T05:23:06.386018Z",
          "iopub.status.idle": "2025-07-09T05:23:14.316359Z",
          "shell.execute_reply": "2025-07-09T05:23:14.315804Z",
          "shell.execute_reply.started": "2025-07-09T05:23:06.386272Z"
        },
        "id": "Xfbq6dazfwer",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoDoByYmFeyv",
        "outputId": "fe0a3b24-521b-4689-a30a-7591b2ed78f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1YEkV7uEf_Ge"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/combined_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-09T05:23:15.633794Z",
          "iopub.status.busy": "2025-07-09T05:23:15.633593Z",
          "iopub.status.idle": "2025-07-09T05:23:15.723954Z",
          "shell.execute_reply": "2025-07-09T05:23:15.723374Z",
          "shell.execute_reply.started": "2025-07-09T05:23:15.633777Z"
        },
        "id": "ebdUT8Xlfwet",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ndata = data.dropna().reset_index(drop=True)  # Reset index here\n",
        "\n",
        "text = ndata['text']\n",
        "labels = ndata['mental_state'].values\n",
        "label_map = {emotion: idx for idx, emotion in enumerate(np.unique(labels))}\n",
        "num_labels = len(label_map)\n",
        "y_encoded = np.array([label_map[l] for l in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "7298b6da413f4e0e9d8db60d3c63e44a",
            "ea429759f0a84660a948fa04b7877b7a",
            "c3f865971bc24f70b58a6befa4788512",
            "fc88fe3f263042a3919b1113d7a9eddb",
            "533527aea58044dd803004e98b7dea60",
            "63545db491a047088e6dde8f5fddf385",
            "85a1b0f97b2b457294664fcdd70ecd8e",
            "ce52927537ee41b58107d2e013bf9ffe",
            "8cbd1df99e3d4054be93d2d6bd1877ed",
            "2db13bc55d7f42db9ba5f3fa167083e0",
            "a3e0b69fa5954aa6b00ef661280dc22a",
            "86f96fd91cee4bf3ac3781cc36192240",
            "e62d609d32034cc58f2365a6f67c0b57",
            "7486fe2e2cf54127a17357b17945e443",
            "924e13d5e7444f658826a6b183c6ce87",
            "b89e37bd6160432b8e1757b0b521a91b",
            "1cd9db49251b42439446d29e175ab07b",
            "8536c1826cde4a538891de4accc99475",
            "b499c7530194442f9c6282d77ea5a0f8",
            "2d39a4dcdece4084b086aba7d6c11ea5",
            "8fbc8619a1054e139ae0826cf32c90c8",
            "3d873a99f0f246d581c20e56f3c568a9",
            "f6dfc9630be24d5aac87c8568f7b354f",
            "02ffe7cef3934b32b759876a33ff52e7",
            "217d2be3263d4724adc103039bf6aa19",
            "cb79d5db67b34f1895bf0f2d2daeb680",
            "5e9a124f9a394736a323f0c223742257",
            "2c7aac3838254f85aa44be7ba9574900",
            "483ca01a93074a3dac876c66cdf65632",
            "d5417492f3314514b753c62a68e8ff6b",
            "7bd6dcd093014d238ad5e3e3506718f7",
            "c8a9cb6952914b24884f4adc219ccf75",
            "a234fffd3b724310a4be46cc17f72449",
            "283989be33464f3aa16c637aad1bd9da",
            "4e6033b0bd0b435ba6072cc2c711fc5c",
            "9388b9a89e3a4668a9607ed8e81a874a",
            "18edd5b4f1f543bcb31cac992675f051",
            "9a36559b642c469a9aac74e32e3e62b6",
            "837b3831b44e41bbb4aa3b9b3ca069c6",
            "1a28449fa70b44828d3d80427b96f060",
            "0529c12848e14c4b98c32f5c26f552f1",
            "acbbe5a189d8410c84995a0e146bb26f",
            "80c7fd16744e4eea89683fcddf28dead",
            "bdba38ecd1a64f01821e6c301e1c58dc",
            "9ee922d86e5d4501ae2ded29696be1a6",
            "bd07b5ce7d9c477694893028d03add96",
            "1a2825014695469db4572b89f13bba96",
            "9338270a937440bfa6af9bda4035627a",
            "2b9edb5cae0644c29c72483e5bd9380d",
            "43e8e745f71a4dabb3164051b18e2132",
            "8d328bd643a64e719802a15bf5ee5f65",
            "bbb05070f90d4526ba598ca6c5814179",
            "5f5ea6b85e104732aa9d8f5c9493b31c",
            "add513b0f4134ad3b59bb1fa8a2e0737",
            "1e102f1599ac47a5865c395c7d04e6c9",
            "6891be6cf22143b28653d57cc95a706b",
            "ce1d241f8f7a4988aeb96cedf43c532a",
            "7a4c2fc46b0a429a93c28270367144bf",
            "4a6c65eb7f444dd98688dbda22f67760",
            "ae227aa4c29549ffb8ec33c40d05a1ac",
            "7259f10562d04984ae46c616ddefe928",
            "13dcc536d46b4ebd8cc04a9f5ab081d5",
            "b3598b5ecf994c9ca54ba49da575cd3f",
            "16a3cf286ffe422eae2a8149a4c504e0",
            "0a8a013b66354baa9921d039b3a33d81",
            "0ec3e2d6e14d4d0daaf0babb0774da31"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-07-09T05:23:16.089476Z",
          "iopub.status.busy": "2025-07-09T05:23:16.089204Z",
          "iopub.status.idle": "2025-07-09T05:23:16.919997Z",
          "shell.execute_reply": "2025-07-09T05:23:16.919207Z",
          "shell.execute_reply.started": "2025-07-09T05:23:16.089454Z"
        },
        "id": "-JjcKCM6fwex",
        "outputId": "2336140d-49b7-487d-da77-d22e84c0e4cc",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7298b6da413f4e0e9d8db60d3c63e44a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86f96fd91cee4bf3ac3781cc36192240",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6dfc9630be24d5aac87c8568f7b354f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "283989be33464f3aa16c637aad1bd9da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ee922d86e5d4501ae2ded29696be1a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6891be6cf22143b28653d57cc95a706b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# With this:\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LzD4t0q8PUOE"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    text, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqkKzpbSP2AB",
        "outputId": "c6326aac-b121-41fa-d4ff-fa94cee05682"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 0/98595 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
            "Tokenizing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98595/98595 [06:41<00:00, 245.35it/s]\n",
            "Tokenizing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24649/24649 [01:33<00:00, 262.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# ===== ENHANCED TOKENIZATION STRATEGIES =====\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "def adaptive_tokenize(text, tokenizer, max_len=512):\n",
        "    \"\"\"\n",
        "    Adaptive tokenization that preserves important content based on text length\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    if len(tokens) <= max_len:\n",
        "        return tokens\n",
        "\n",
        "    # Strategy 1: For very long texts, use sliding window approach\n",
        "    if len(tokens) > max_len * 3:\n",
        "        # Take beginning, middle, and end chunks\n",
        "        chunk_size = max_len // 3\n",
        "        beginning = tokens[:chunk_size]\n",
        "        middle_start = len(tokens) // 2 - chunk_size // 2\n",
        "        middle = tokens[middle_start:middle_start + chunk_size]\n",
        "        end = tokens[-chunk_size:]\n",
        "        return beginning + middle + end\n",
        "\n",
        "    # Strategy 2: For moderately long texts, use head-tail with bias toward beginning\n",
        "    else:\n",
        "        head_ratio = 0.6  # Give more weight to beginning\n",
        "        head_len = int(max_len * head_ratio)\n",
        "        tail_len = max_len - head_len\n",
        "        head = tokens[:head_len]\n",
        "        tail = tokens[-tail_len:]\n",
        "        return head + tail\n",
        "\n",
        "def enhanced_pre_tokenize(texts, labels, tokenizer, max_len=512):\n",
        "    \"\"\"Enhanced tokenization with multiple strategies\"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        tokens = adaptive_tokenize(text, tokenizer, max_len)\n",
        "\n",
        "        # Ensure exact length\n",
        "        if len(tokens) < max_len:\n",
        "            tokens = tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:max_len]\n",
        "\n",
        "        mask = [1 if t != tokenizer.pad_token_id else 0 for t in tokens]\n",
        "        input_ids.append(tokens)\n",
        "        attention_masks.append(mask)\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor(input_ids),\n",
        "        'attention_mask': torch.tensor(attention_masks),\n",
        "        'labels': torch.tensor(labels)\n",
        "    }\n",
        "\n",
        "# Enhanced tokenization\n",
        "MAX_LEN = 512\n",
        "train_data = enhanced_pre_tokenize(X_train.tolist(), y_train, tokenizer, MAX_LEN)\n",
        "val_data = enhanced_pre_tokenize(X_val.tolist(), y_val, tokenizer, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6miSP5y4fwey",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Handle class imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-WCG0o5Vfwez",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Weighted Focal Loss\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, gamma=2, weights=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weights = weights\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = targets.to(self.device)\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.weights is not None:\n",
        "            weights = self.weights[targets]\n",
        "            weighted_loss = focal_loss * weights\n",
        "            return weighted_loss.mean()\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Initialize loss function\n",
        "loss_fn = WeightedFocalLoss(alpha=0.5, gamma=3, weights=weights_tensor, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cAn7tj586zbz"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.optim import AdamW\n",
        "def get_optimizer_grouped_parameters(model, base_lr=2e-5, lr_decay=0.95):\n",
        "    # For DistilBERT, there are 6 transformer layers: layer.0 ... layer.5\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = []\n",
        "    # Classifier head\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n],\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr\": base_lr,\n",
        "    })\n",
        "    # Transformer layers (from last to first, higher lr for higher layers)\n",
        "    for i in range(5, -1, -1):\n",
        "        lr = base_lr * (lr_decay ** (5 - i))\n",
        "        optimizer_grouped_parameters.append({\n",
        "            \"params\": [p for n, p in model.named_parameters() if f\"layer.{i}.\" in n and not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.01,\n",
        "            \"lr\": lr,\n",
        "        })\n",
        "        optimizer_grouped_parameters.append({\n",
        "            \"params\": [p for n, p in model.named_parameters() if f\"layer.{i}.\" in n and any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": lr,\n",
        "        })\n",
        "    # Embeddings\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.named_parameters() if \"embeddings\" in n],\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr\": base_lr * (lr_decay ** 6),\n",
        "    })\n",
        "    return optimizer_grouped_parameters\n",
        "\n",
        "# Usage example:\n",
        "optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, base_lr=2e-5, lr_decay=0.95)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VVuDKHvDfwez",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Replace TensorDataset with this custom dictionary-style dataset\n",
        "class PrecomputedDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5UhtB84Xfwe0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ===== FIXED UNFREEZING STRATEGY =====\n",
        "def unfreeze_layers(model, current_epoch, freeze_at_epoch=3):\n",
        "    \"\"\"Gradually unfreeze layers during training\"\"\"\n",
        "    # Initial freezing - freeze all except classifier and last layer\n",
        "    if current_epoch == 0:\n",
        "        print(\"Initial freezing: Only classifier and last layer are trainable\")\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'classifier' not in name and 'layer.5' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Unfreeze additional layers after certain epochs\n",
        "    if current_epoch >= freeze_at_epoch:\n",
        "        print(f\"Epoch {current_epoch}: Unfreezing layers 4 and 5\")\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'layer.4' in name or 'layer.5' in name or 'classifier' in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "class CustomLossTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_fn=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.label_names = [\"labels\"]\n",
        "        self._current_epoch = -1  # Track current epoch internally\n",
        "\n",
        "    def create_optimizer(self):\n",
        "        if self.optimizer is None:\n",
        "            optimizer_grouped_parameters = get_optimizer_grouped_parameters(\n",
        "                self.model, base_lr=self.args.learning_rate, lr_decay=0.95\n",
        "            )\n",
        "            self.optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
        "        return self.optimizer\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        self._current_epoch += 1\n",
        "        if hasattr(self, 'model'):\n",
        "            self.model = unfreeze_layers(self.model, self._current_epoch)\n",
        "        return super().train(*args, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Wk_bQZ_QNCG"
      },
      "outputs": [],
      "source": [
        "# Create datasets using the new class\n",
        "train_dataset = PrecomputedDataset(\n",
        "    train_data['input_ids'],\n",
        "    train_data['attention_mask'],\n",
        "    train_data['labels']\n",
        ")\n",
        "\n",
        "val_dataset = PrecomputedDataset(\n",
        "    val_data['input_ids'],\n",
        "    val_data['attention_mask'],\n",
        "    val_data['labels']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "iJXc782igCt5",
        "outputId": "cfc395e4-5b64-4c75-ce4c-ea3b38f7bc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial freezing: Only classifier and last layer are trainable\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='52256' max='73950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [52256/73950 1:53:56 < 47:18, 7.64 it/s, Epoch 4.24/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.307200</td>\n",
              "      <td>0.272885</td>\n",
              "      <td>0.736582</td>\n",
              "      <td>0.736482</td>\n",
              "      <td>0.755229</td>\n",
              "      <td>0.736582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.238700</td>\n",
              "      <td>0.252421</td>\n",
              "      <td>0.760599</td>\n",
              "      <td>0.761481</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.760599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.225215</td>\n",
              "      <td>0.752607</td>\n",
              "      <td>0.752667</td>\n",
              "      <td>0.774269</td>\n",
              "      <td>0.752607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.172400</td>\n",
              "      <td>0.259248</td>\n",
              "      <td>0.755852</td>\n",
              "      <td>0.756294</td>\n",
              "      <td>0.785415</td>\n",
              "      <td>0.755852</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='73950' max='73950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [73950/73950 2:42:28, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.307200</td>\n",
              "      <td>0.272885</td>\n",
              "      <td>0.736582</td>\n",
              "      <td>0.736482</td>\n",
              "      <td>0.755229</td>\n",
              "      <td>0.736582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.238700</td>\n",
              "      <td>0.252421</td>\n",
              "      <td>0.760599</td>\n",
              "      <td>0.761481</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.760599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.225215</td>\n",
              "      <td>0.752607</td>\n",
              "      <td>0.752667</td>\n",
              "      <td>0.774269</td>\n",
              "      <td>0.752607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.172400</td>\n",
              "      <td>0.259248</td>\n",
              "      <td>0.755852</td>\n",
              "      <td>0.756294</td>\n",
              "      <td>0.785415</td>\n",
              "      <td>0.755852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.134600</td>\n",
              "      <td>0.285355</td>\n",
              "      <td>0.767414</td>\n",
              "      <td>0.768021</td>\n",
              "      <td>0.785258</td>\n",
              "      <td>0.767414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.123800</td>\n",
              "      <td>0.283170</td>\n",
              "      <td>0.772567</td>\n",
              "      <td>0.773532</td>\n",
              "      <td>0.785364</td>\n",
              "      <td>0.772567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1541' max='1541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1541/1541 02:48]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results: {'eval_loss': 0.28317025303840637, 'eval_accuracy': 0.7725668384112946, 'eval_f1': 0.7735318389182788, 'eval_precision': 0.7853637185507724, 'eval_recall': 0.7725668384112946, 'eval_runtime': 168.3507, 'eval_samples_per_second': 146.415, 'eval_steps_per_second': 9.154, 'epoch': 6.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/bert_results_jiuhu/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/bert_results_jiuhu/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/bert_results_jiuhu/vocab.json',\n",
              " '/content/drive/MyDrive/bert_results_jiuhu/merges.txt',\n",
              " '/content/drive/MyDrive/bert_results_jiuhu/added_tokens.json')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy='epoch',  # Corrected from 'eval_strategy'\n",
        "    save_strategy='epoch',\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir='./logs',\n",
        "    report_to='none',\n",
        "    dataloader_pin_memory=True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Trainer initialization\n",
        "trainer = CustomLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\n",
        "        'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
        "        'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted'),\n",
        "        'precision': precision_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted'),\n",
        "        'recall': recall_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted')\n",
        "    },\n",
        "    loss_fn=loss_fn\n",
        ")\n",
        "\n",
        "# Train & Evaluate\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(\"Validation Results:\", results)\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.save_model('./results')\n",
        "tokenizer.save_pretrained('./results')\n",
        "save_path = \"/content/drive/MyDrive/bert_results_jiuhu\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHeSQ2Ahc22r",
        "outputId": "999760f3-798a-485e-cb2f-cf973bdf6d6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/bert_results_2026 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/drive/MyDrive/bert_results_2026\"\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
        "model = RobertaModel.from_pretrained(model_path).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRW3xSkHGjhU"
      },
      "outputs": [],
      "source": [
        "def get_embeddings_mean_max_chunked(texts, tokenizer, model, device, max_length=512, stride=256):\n",
        "    if isinstance(texts, pd.Series):\n",
        "        texts = texts.tolist()\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    all_embeddings = []\n",
        "    hidden_size = getattr(model.config, \"dim\", None) or getattr(model.config, \"hidden_size\", 768)\n",
        "    for text in texts:\n",
        "        # Use tokenizer's built-in chunking\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            stride=stride,\n",
        "            return_overflowing_tokens=True\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "        chunk_embeddings = []\n",
        "        for i in range(input_ids.size(0)):\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids=input_ids[i:i+1], attention_mask=attention_mask[i:i+1])\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask = attention_mask[i:i+1].unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            mean_pool = torch.sum(last_hidden * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "            max_pool = torch.max(last_hidden.masked_fill(mask == 0, -1e9), 1)[0]\n",
        "            pooled = torch.cat([mean_pool, max_pool], dim=1)\n",
        "            chunk_embeddings.append(pooled.cpu().numpy())\n",
        "        if chunk_embeddings:\n",
        "            all_embeddings.append(np.mean(np.vstack(chunk_embeddings), axis=0))\n",
        "        else:\n",
        "            all_embeddings.append(np.zeros((hidden_size * 2,)))\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "embeddings = get_embeddings_mean_max_chunked(text, tokenizer, model, device)\n",
        "np.save('/content/drive/MyDrive/roberta_embeddings.npy', embeddings)\n",
        "pd.DataFrame(embeddings).to_csv('/content/drive/MyDrive/roberta_embeddings.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akLTD7mbX_77"
      },
      "outputs": [],
      "source": [
        "embeddings = np.load('/content/drive/MyDrive/roberta_embeddings.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnHfm0RgPUOK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7744210,
          "sourceId": 12287966,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7751826,
          "sourceId": 12298920,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7828181,
          "sourceId": 12412430,
          "sourceType": "datasetVersion"
        },
        {
          "modelId": 395855,
          "modelInstanceId": 375023,
          "sourceId": 464409,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}